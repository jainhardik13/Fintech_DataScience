{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T06:37:45.844323100Z",
     "start_time": "2026-02-09T06:37:45.808715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Build a tool to scrape the web and extract data from HTML documents.\n",
    "# Web scraping resume from AWS S3 to extract skills section\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n"
   ],
   "id": "f6f2b99433ced98a",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T06:37:50.956233Z",
     "start_time": "2026-02-09T06:37:45.861322400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 1: Fetch the webpage from S3\n",
    "# Replace with your actual S3 URL\n",
    "# resume_url = \"http://hardik-jain-1837.s3-website-us-east-1.amazonaws.com/\"\n",
    "# resume_url = \"https://krishaggarwal-s3-us-east-1-resume.s3.us-east-1.amazonaws.com/resume.html\"\n",
    "resume_url = input(\"Enter link to resume webpage (S3 URL): \")\n",
    "# e.g., \"https://your-bucket.s3.amazonaws.com/resume.html\"\n",
    "\n",
    "try:\n",
    "    # Send GET request to fetch the webpage\n",
    "    response = requests.get(resume_url)\n",
    "    response.raise_for_status()  # Raise an error for bad status codes\n",
    "\n",
    "    print(f\"Successfully fetched webpage. Status Code: {response.status_code}\")\n",
    "    print(f\"Content Length: {len(response.content)} bytes\")\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error fetching webpage: {e}\")\n",
    "    response = None\n"
   ],
   "id": "9bca0d2400f1cd4f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully fetched webpage. Status Code: 200\n",
      "Content Length: 9717 bytes\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T06:37:51.022004600Z",
     "start_time": "2026-02-09T06:37:50.974304100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 2: Parse the HTML content\n",
    "if response:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Pretty print the HTML structure (first 1000 characters)\n",
    "    print(\"HTML Structure Preview:\")\n",
    "    print(soup.prettify()[:1000])\n"
   ],
   "id": "efc7c52c41069a83",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML Structure Preview:\n",
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      " <head>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <meta content=\"width=device-width, initial-scale=1.0\" name=\"viewport\"/>\n",
      "  <title>\n",
      "   Kartikeya Jain - Resume\n",
      "  </title>\n",
      "  <style>\n",
      "   * {\r\n",
      "            margin: 0;\r\n",
      "            padding: 0;\r\n",
      "            box-sizing: border-box;\r\n",
      "        }\r\n",
      "\r\n",
      "        body {\r\n",
      "            font-family: 'Georgia', 'Times New Roman', serif;\r\n",
      "            line-height: 1.5;\r\n",
      "            color: #000;\r\n",
      "            background-color: #525659;\r\n",
      "            padding: 30px;\r\n",
      "        }\r\n",
      "\r\n",
      "        .container {\r\n",
      "            max-width: 8.5in;\r\n",
      "            min-height: 11in;\r\n",
      "            margin: 0 auto;\r\n",
      "            background-color: #fff;\r\n",
      "            padding: 0.75in;\r\n",
      "            box-shadow: 0 0 30px rgba(0, 0, 0, 0.3);\r\n",
      "        }\r\n",
      "\r\n",
      "        h1 {\r\n",
      "            font-size: 28px;\r\n",
      "            color: #000;\r\n",
      "            margin-bottom: 8px;\r\n",
      "            text-align: center;\r\n",
      "            letter-spacing: 2px;\r\n",
      "            font-weight: 700;\r\n",
      "            text-transform: u\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T06:37:51.045610700Z",
     "start_time": "2026-02-09T06:37:51.025008200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 3: Extract Skills Section\n",
    "# This will try multiple common patterns for skills sections\n",
    "\n",
    "def extract_skills(soup):\n",
    "    \"\"\"\n",
    "    Extract skills from resume webpage using multiple strategies\n",
    "    \"\"\"\n",
    "    skills = []\n",
    "\n",
    "    # Strategy 1: Look for sections with 'skill' in id or class\n",
    "    skills_section = (\n",
    "        soup.find(id=lambda x: x and 'skill' in x.lower()) or\n",
    "        soup.find(class_=lambda x: x and 'skill' in str(x).lower()) or\n",
    "        soup.find('section', class_=lambda x: x and 'skill' in str(x).lower()) or\n",
    "        soup.find('div', class_=lambda x: x and 'skill' in str(x).lower())\n",
    "    )\n",
    "\n",
    "    if skills_section:\n",
    "        print(\"Found skills section using Strategy 1 (id/class matching)\")\n",
    "\n",
    "        # Extract text from list items\n",
    "        list_items = skills_section.find_all(['li', 'span', 'p'])\n",
    "        for item in list_items:\n",
    "            skill_text = item.get_text(strip=True)\n",
    "            if skill_text and len(skill_text) > 0:\n",
    "                skills.append(skill_text)\n",
    "\n",
    "        # If no list items, get all text\n",
    "        if not skills:\n",
    "            skills_text = skills_section.get_text(separator='\\n', strip=True)\n",
    "            skills = [line.strip() for line in skills_text.split('\\n') if line.strip()]\n",
    "\n",
    "    # Strategy 2: Look for headings containing 'skill'\n",
    "    if not skills:\n",
    "        print(\"Trying Strategy 2 (heading-based search)\")\n",
    "        headings = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "        for heading in headings:\n",
    "            if 'skill' in heading.get_text().lower():\n",
    "                # Get the next sibling elements\n",
    "                next_element = heading.find_next_sibling()\n",
    "                while next_element:\n",
    "                    if next_element.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:\n",
    "                        break\n",
    "                    if next_element.name in ['ul', 'ol']:\n",
    "                        list_items = next_element.find_all('li')\n",
    "                        skills.extend([item.get_text(strip=True) for item in list_items])\n",
    "                    elif next_element.name in ['p', 'div']:\n",
    "                        text = next_element.get_text(strip=True)\n",
    "                        if text:\n",
    "                            skills.append(text)\n",
    "                    next_element = next_element.find_next_sibling()\n",
    "                if skills:\n",
    "                    break\n",
    "\n",
    "    # Strategy 3: Search for all lists and filter\n",
    "    if not skills:\n",
    "        print(\"Trying Strategy 3 (comprehensive list search)\")\n",
    "        all_lists = soup.find_all(['ul', 'ol'])\n",
    "        for lst in all_lists:\n",
    "            # Check if parent or previous sibling mentions skills\n",
    "            context = \"\"\n",
    "            if lst.find_previous(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
    "                context = lst.find_previous(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']).get_text().lower()\n",
    "\n",
    "            if 'skill' in context:\n",
    "                list_items = lst.find_all('li')\n",
    "                skills.extend([item.get_text(strip=True) for item in list_items])\n",
    "                break\n",
    "\n",
    "    return skills\n"
   ],
   "id": "20c0dd999395e595",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T06:37:51.071912200Z",
     "start_time": "2026-02-09T06:37:51.047610300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 4: Execute extraction and display results\n",
    "if response:\n",
    "    extracted_skills = extract_skills(soup)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"EXTRACTED SKILLS\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    if extracted_skills:\n",
    "        for idx, skill in enumerate(extracted_skills, 1):\n",
    "            print(f\"{idx}. {skill}\")\n",
    "\n",
    "        print(f\"\\nTotal skills found: {len(extracted_skills)}\")\n",
    "    else:\n",
    "        print(\"No skills found. The HTML structure might be different.\")\n",
    "        print(\"\\nTip: Inspect the HTML structure manually to identify the skills section.\")\n",
    "        print(\"You can view the full HTML by uncommenting the line below:\")\n",
    "        print(\"# print(soup.prettify())\")\n",
    "\n",
    "    print(\"=\"*50)\n"
   ],
   "id": "b26912bdcdff5066",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found skills section using Strategy 1 (id/class matching)\n",
      "\n",
      "==================================================\n",
      "EXTRACTED SKILLS\n",
      "==================================================\n",
      "1. Programming Languages:\n",
      "2. Java, C++, C, Python\n",
      "3. Frontend:\n",
      "4. HTML, CSS, JavaScript, React\n",
      "5. Backend:\n",
      "6. Node.js, Express.js, Spring Boot\n",
      "7. Databases:\n",
      "8. MySQL, MongoDB (Cloud Atlas)\n",
      "9. Core Concepts:\n",
      "10. Data Structures & Algorithms, OOP, DBMS, Operating Systems, Computer Networks\n",
      "11. Tools/Cloud:\n",
      "12. AWS, Git, REST APIs\n",
      "\n",
      "Total skills found: 12\n",
      "==================================================\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T06:37:51.102396800Z",
     "start_time": "2026-02-09T06:37:51.075915100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 5: Save skills to a CSV file\n",
    "if response and extracted_skills:\n",
    "    # Create a DataFrame\n",
    "    skills_df = pd.DataFrame({\n",
    "        'Skill_Number': range(1, len(extracted_skills) + 1),\n",
    "        'Skill': extracted_skills\n",
    "    })\n",
    "\n",
    "    # Save to CSV\n",
    "    output_file = 'extracted_skills.csv'\n",
    "    skills_df.to_csv(output_file, index=False)\n",
    "    print(f\"\\nSkills saved to: {output_file}\")\n",
    "\n",
    "    # Display the DataFrame\n",
    "    print(\"\\nSkills DataFrame:\")\n",
    "    print(skills_df)\n"
   ],
   "id": "8f8fd369d85099a4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Skills saved to: extracted_skills.csv\n",
      "\n",
      "Skills DataFrame:\n",
      "    Skill_Number                                              Skill\n",
      "0              1                             Programming Languages:\n",
      "1              2                               Java, C++, C, Python\n",
      "2              3                                          Frontend:\n",
      "3              4                       HTML, CSS, JavaScript, React\n",
      "4              5                                           Backend:\n",
      "5              6                   Node.js, Express.js, Spring Boot\n",
      "6              7                                         Databases:\n",
      "7              8                       MySQL, MongoDB (Cloud Atlas)\n",
      "8              9                                     Core Concepts:\n",
      "9             10  Data Structures & Algorithms, OOP, DBMS, Opera...\n",
      "10            11                                       Tools/Cloud:\n",
      "11            12                                AWS, Git, REST APIs\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T06:37:51.117373500Z",
     "start_time": "2026-02-09T06:37:51.105397100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 6: Advanced extraction - Get all sections\n",
    "def extract_all_sections(soup):\n",
    "    \"\"\"\n",
    "    Extract all major sections from the resume\n",
    "    \"\"\"\n",
    "    sections = {}\n",
    "\n",
    "    # Find all headings\n",
    "    headings = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "\n",
    "    for heading in headings:\n",
    "        section_title = heading.get_text(strip=True)\n",
    "        section_content = []\n",
    "\n",
    "        # Get content until next heading\n",
    "        next_element = heading.find_next_sibling()\n",
    "        while next_element and next_element.name not in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:\n",
    "            text = next_element.get_text(strip=True)\n",
    "            if text:\n",
    "                section_content.append(text)\n",
    "            next_element = next_element.find_next_sibling()\n",
    "\n",
    "        if section_content:\n",
    "            sections[section_title] = section_content\n",
    "\n",
    "    return sections\n"
   ],
   "id": "bbb311a9051bc230",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T06:37:51.147211300Z",
     "start_time": "2026-02-09T06:37:51.120372800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Optional: Extract all sections for complete resume data\n",
    "if response:\n",
    "    all_sections = extract_all_sections(soup)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ALL RESUME SECTIONS\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    for section_title, content in all_sections.items():\n",
    "        print(f\"\\n### {section_title} ###\")\n",
    "        for item in content:\n",
    "            print(f\"  - {item}\")\n",
    "\n",
    "    print(\"=\"*50)\n",
    "\n"
   ],
   "id": "cb6f1e46106d0f52",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ALL RESUME SECTIONS\n",
      "==================================================\n",
      "\n",
      "### KARTIKEYA JAIN ###\n",
      "  - jainkartikeya9@gmail.com•9499166339•linkedin.com/in/kartikeyajain1911•github.com/kartikeya1911\n",
      "\n",
      "### Profile ###\n",
      "  - Full-stack developer with hands-on experience building real-time MERN applications, scalable Spring Boot APIs, and AI-powered platforms. Strong in Data Structures & Algorithms, backend optimization, and cloud-based systems with proven hackathon success and production-ready project experience. Currently expanding expertise in Python-based data science libraries for analytics and machine learning.\n",
      "\n",
      "### Education ###\n",
      "  - Chitkara UniversityRajpura, PunjabB.E. Computer Science and Engineering2023 - 2027CGPA: 8.72\n",
      "\n",
      "### Skills ###\n",
      "  - Programming Languages:Java, C++, C, PythonFrontend:HTML, CSS, JavaScript, ReactBackend:Node.js, Express.js, Spring BootDatabases:MySQL, MongoDB (Cloud Atlas)Core Concepts:Data Structures & Algorithms, OOP, DBMS, Operating Systems, Computer NetworksTools/Cloud:AWS, Git, REST APIs\n",
      "\n",
      "### Projects ###\n",
      "  - ColabCanvas – Real-Time Collaborative Platform (MERN + WebSockets)Built a real-time collaborative note-sharing web application supporting multi-user live editingEngineered Socket.IO–based synchronization, reducing content update latency by ~40%Designed secure JWT-based authentication with role-based access controlEnhanced backend performance for concurrent users using MongoDB Atlas cloud infrastructureIntegrated rich-text editing capabilities and PDF export functionalityAI Resume Builder – ATS Optimized Platform (MERN + AI APIs)Developed an intelligent resume generation system optimized for Applicant Tracking SystemsImplemented skill matching and keyword optimization improving job-fit scoreBuilt secure user authentication and cloud-based profile managementEnforced resume analytics to suggest improvements based on job descriptionsServeNow – Full-stack Food Delivery System (Java + Spring Boot)Developed scalable RESTful APIs using Spring Boot following MVC architectureIntegrated Spring Security with JWT for secure role-based authenticationDesigned an optimized MySQL database schema for users, orders, and menu managementImproved API response times through query optimization and backend refactoringExecuted order lifecycle tracking with real-time status updates for users and administrators\n",
      "\n",
      "### Achievements & Leadership ###\n",
      "  - Solved 300+ DSA problems on LeetCode & GeeksforGeeks (arrays, trees, graphs, DP)Winner of university hackathon (50+ teams) for building real-time web solution in 24 hoursParticipated in national & university hackathons developing full-stack applicationsActive NSS volunteer and student team lead managing development & deployment\n",
      "==================================================\n"
     ]
    }
   ],
   "execution_count": 26
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
